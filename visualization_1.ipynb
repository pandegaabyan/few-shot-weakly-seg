{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from scipy import stats\n",
    "\n",
    "alt.data_transformers.enable(\"vegafusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wandb_table(path: str) -> pd.DataFrame:\n",
    "    import json\n",
    "\n",
    "    with open(path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    columns = data[\"columns\"]\n",
    "    rows = data[\"data\"]\n",
    "    return pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# from utils.wandb import wandb_path\n",
    "\n",
    "# runs = wandb.Api().runs(\n",
    "#     wandb_path(False),\n",
    "#     filters={\"jobType\": \"test\"},\n",
    "# )\n",
    "\n",
    "# for i, run in enumerate(runs):\n",
    "#     run_id = run.name.split(\" \")[-1]\n",
    "#     dataset = run.config[\"test_dataset\"].replace(\"-test\", \"\")\n",
    "#     group = run.group\n",
    "#     if group == \"SL\":\n",
    "#         pass\n",
    "#     elif len(group.split(\"-\")) == 1:\n",
    "#         group += \"-new\"\n",
    "#     elif group.split(\"-\")[1] == \"b\":\n",
    "#         group = group.replace(\"-b\", \"-new-b\")\n",
    "#     run.logged_artifacts()[2].download(f\"logs/wandb/1_metrics/{group} {dataset} {run_id}\")\n",
    "#     print(group, dataset, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# df_list = []\n",
    "\n",
    "# wandb_dir = \"logs/wandb/1_metrics\"\n",
    "# for dir in os.listdir(wandb_dir):\n",
    "#     if os.path.isfile(f\"{wandb_dir}/{dir}\"):\n",
    "#         continue\n",
    "#     if not dir.startswith(\"SL\"):\n",
    "#         continue\n",
    "#     if dir.split(\" \")[-1] not in [\"ZAr\", \"eMD\", \"N8k\"]:\n",
    "#         continue\n",
    "#     _, dataset, _ = dir.split(\" \")\n",
    "#     df = read_wandb_table(f\"{wandb_dir}/{dir}/metrics.table.json\")\n",
    "#     df.drop(columns=[\"type\", \"epoch\"], inplace=True)\n",
    "#     df.insert(0, \"dataset\", dataset)\n",
    "#     df_list.append(df)\n",
    "\n",
    "# simple_metrics_df = pd.concat(df_list)\n",
    "# simple_metrics_df.to_csv(\"logs/wandb/1_simple_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# df_list = []\n",
    "\n",
    "# wandb_dir = \"logs/wandb/1_metrics\"\n",
    "# for dir in os.listdir(wandb_dir):\n",
    "#     if os.path.isfile(f\"{wandb_dir}/{dir}\"):\n",
    "#         continue\n",
    "#     if dir.startswith(\"SL\"):\n",
    "#         continue\n",
    "#     group, dataset, _ = dir.split(\" \")\n",
    "#     df = read_wandb_table(f\"{wandb_dir}/{dir}/metrics.table.json\")\n",
    "#     df.drop(columns=[\"type\", \"epoch\"], inplace=True)\n",
    "#     df.insert(0, \"dataset\", dataset)\n",
    "#     df.insert(0, \"method\", group)\n",
    "#     df_list.append(df)\n",
    "\n",
    "# meta_metrics_df = pd.concat(df_list)\n",
    "# meta_metrics_df.to_csv(\"logs/wandb/1_meta_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_metrics_df = pd.read_csv(\"logs/wandb/1_simple_metrics.csv\")\n",
    "simple_metrics_df.insert(0, \"method\", \"SL\")\n",
    "\n",
    "simple_metrics_df[\"iou_cup\"] = simple_metrics_df[\"iou_cup\"] * 100\n",
    "simple_metrics_df[\"iou_disc\"] = simple_metrics_df[\"iou_disc\"] * 100\n",
    "\n",
    "simple_metrics_df[\"iou\"] = (\n",
    "    simple_metrics_df[\"iou_cup\"] + simple_metrics_df[\"iou_disc\"]\n",
    ") / 2\n",
    "\n",
    "simple_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_metrics_df = pd.read_csv(\"logs/wandb/1_meta_metrics.csv\")\n",
    "meta_metrics_df = meta_metrics_df[~meta_metrics_df[\"method\"].str.endswith(\"-B\")]\n",
    "\n",
    "meta_metrics_df[\"iou_cup\"] = meta_metrics_df[\"iou_cup\"] * 100\n",
    "meta_metrics_df[\"iou_disc\"] = meta_metrics_df[\"iou_disc\"] * 100\n",
    "\n",
    "meta_metrics_df[\"iou\"] = (meta_metrics_df[\"iou_cup\"] + meta_metrics_df[\"iou_disc\"]) / 2\n",
    "\n",
    "meta_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_metrics(target_column: str, use_best: bool) -> pd.DataFrame:\n",
    "    if use_best:\n",
    "        comparison_df = (\n",
    "            pd.concat(\n",
    "                [\n",
    "                    simple_metrics_df[[\"dataset\", \"method\", \"iou\", target_column]],\n",
    "                    meta_metrics_df[\n",
    "                        [\n",
    "                            \"dataset\",\n",
    "                            \"method\",\n",
    "                            \"shot\",\n",
    "                            \"sparsity_mode\",\n",
    "                            \"sparsity_value\",\n",
    "                            \"iou\",\n",
    "                            target_column,\n",
    "                        ]\n",
    "                    ],\n",
    "                ]\n",
    "            )\n",
    "            .rename(columns={\"iou\": \"iou_ref\"})\n",
    "            .groupby(\n",
    "                [\"dataset\", \"method\", \"shot\", \"sparsity_mode\", \"sparsity_value\"],\n",
    "                dropna=False,\n",
    "            )\n",
    "            .agg(\n",
    "                iou_ref=(\"iou_ref\", \"mean\"),\n",
    "                iou=(target_column, \"mean\"),\n",
    "                iou_std=(target_column, \"std\"),\n",
    "                iou_count=(target_column, \"count\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        comparison_df = comparison_df.loc[\n",
    "            comparison_df.groupby([\"dataset\", \"method\"])[\"iou_ref\"].idxmax()\n",
    "        ]\n",
    "    else:\n",
    "        comparison_df = (\n",
    "            pd.concat(\n",
    "                [\n",
    "                    simple_metrics_df[[\"dataset\", \"method\", target_column]],\n",
    "                    meta_metrics_df[[\"dataset\", \"method\", target_column]],\n",
    "                ]\n",
    "            )\n",
    "            .groupby([\"dataset\", \"method\"])\n",
    "            .agg(\n",
    "                iou=(target_column, \"mean\"),\n",
    "                iou_std=(target_column, \"std\"),\n",
    "                iou_count=(target_column, \"count\"),\n",
    "            )\n",
    "        ).reset_index()\n",
    "\n",
    "    comparison_df[\"iou_std_err\"] = (\n",
    "        comparison_df[\"iou_std\"] / comparison_df[\"iou_count\"] ** 0.5\n",
    "    )\n",
    "    comparison_df[\"iou_low\"] = (\n",
    "        comparison_df[\"iou\"] - 1.96 * comparison_df[\"iou_std_err\"]\n",
    "    )\n",
    "    comparison_df[\"iou_high\"] = (\n",
    "        comparison_df[\"iou\"] + 1.96 * comparison_df[\"iou_std_err\"]\n",
    "    )\n",
    "\n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = compare_metrics(\"iou\", False)\n",
    "disc_comparison_df = compare_metrics(\"iou_disc\", False)\n",
    "cup_comparison_df = compare_metrics(\"iou_cup\", False)\n",
    "# best_comparison_df = compare_metrics(\"iou\", True)\n",
    "best_disc_comparison_df = compare_metrics(\"iou_disc\", True)\n",
    "best_cup_comparison_df = compare_metrics(\"iou_cup\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods_with_sl_by_dataset(df):\n",
    "    \"\"\"\n",
    "    Performs one-tailed t-tests comparing SL method with all other methods within each dataset.\n",
    "    H0: μ_SL <= μ_other\n",
    "    H1: μ_SL > μ_other\n",
    "\n",
    "    Parameters:\n",
    "    df: DataFrame containing columns 'dataset', 'method', 'iou', 'iou_std', 'iou_count'\n",
    "\n",
    "    Returns:\n",
    "    DataFrame with comparison results\n",
    "    \"\"\"\n",
    "    # Initialize results list\n",
    "    results = []\n",
    "\n",
    "    # Group by dataset and process each separately\n",
    "    for dataset in df[\"dataset\"].unique():\n",
    "        dataset_df = df[df[\"dataset\"] == dataset]\n",
    "\n",
    "        # Get SL method statistics for this dataset\n",
    "        sl_stats = dataset_df[dataset_df[\"method\"] == \"SL\"].iloc[0]\n",
    "        sl_mean = sl_stats[\"iou\"]\n",
    "        sl_std = sl_stats[\"iou_std\"]\n",
    "        sl_n = sl_stats[\"iou_count\"]\n",
    "\n",
    "        # Compare SL with each other method in this dataset\n",
    "        for _, row in dataset_df[dataset_df[\"method\"] != \"SL\"].iterrows():\n",
    "            other_mean = row[\"iou\"]\n",
    "            other_std = row[\"iou_std\"]\n",
    "            other_n = row[\"iou_count\"]\n",
    "\n",
    "            # Calculate pooled standard error\n",
    "            s_p = np.sqrt((sl_std**2 / sl_n) + (other_std**2 / other_n))\n",
    "\n",
    "            # Calculate t-statistic\n",
    "            t_stat = (sl_mean - other_mean) / s_p\n",
    "\n",
    "            # Calculate degrees of freedom using Welch-Satterthwaite equation\n",
    "            df_num = (sl_std**2 / sl_n + other_std**2 / other_n) ** 2\n",
    "            df_denom = (sl_std**4) / (sl_n**2 * (sl_n - 1)) + (other_std**4) / (\n",
    "                other_n**2 * (other_n - 1)\n",
    "            )\n",
    "            df_welch = df_num / df_denom\n",
    "\n",
    "            # Calculate one-tailed p-value\n",
    "            p_value = 1 - stats.t.cdf(t_stat, df_welch)\n",
    "\n",
    "            # Store results\n",
    "            results.append(\n",
    "                {\n",
    "                    \"dataset\": dataset,\n",
    "                    \"method_compared\": row[\"method\"],\n",
    "                    \"sl_iou\": sl_mean,\n",
    "                    \"method_iou\": other_mean,\n",
    "                    \"iou_diff\": sl_mean - other_mean,\n",
    "                    \"t_statistic\": t_stat,\n",
    "                    \"degrees_of_freedom\": df_welch,\n",
    "                    \"p_value\": p_value,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Create DataFrame and sort by dataset and p-value\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df = result_df.sort_values([\"dataset\", \"p_value\"])\n",
    "\n",
    "    # Round numerical columns for better readability\n",
    "    numeric_cols = [\n",
    "        \"sl_iou\",\n",
    "        \"method_iou\",\n",
    "        \"iou_diff\",\n",
    "        \"t_statistic\",\n",
    "        \"degrees_of_freedom\",\n",
    "        \"p_value\",\n",
    "    ]\n",
    "    result_df[numeric_cols] = result_df[numeric_cols].round(6)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# compare_methods_with_sl_by_dataset(best_comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_methods = [\n",
    "    \"ProtoSeg\",\n",
    "    \"O-ProtoSeg\",\n",
    "    \"EO-ProtoSeg\",\n",
    "    \"SL\",\n",
    "    \"WeaSeL\",\n",
    "    \"O-WeaSeL\",\n",
    "    \"EO-WeaSeL\",\n",
    "]\n",
    "\n",
    "color_scale = alt.Scale(\n",
    "    domain=ordered_methods,\n",
    "    range=[\n",
    "        \"#ffda03\",  # Yellow\n",
    "        \"#e85d04\",  # Orange\n",
    "        \"#d00000\",  # Red\n",
    "        \"#757575\",  # Gray\n",
    "        \"#43b0f1\",  # Blue\n",
    "        \"#2ec4b6\",  # Turquoise\n",
    "        \"#2d6a4f\",  # Green\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_bar_chart(\n",
    "    dataframes: list[tuple[pd.DataFrame, str]], scale: tuple[float, float] | None = None\n",
    "):\n",
    "    new_dataframes = []\n",
    "    for df in dataframes:\n",
    "        new_df = df[0].copy()\n",
    "        new_df[\"iou_type\"] = df[1]\n",
    "        new_dataframes.append(new_df)\n",
    "    data = pd.concat(new_dataframes)\n",
    "\n",
    "    base = alt.Chart(data).encode(\n",
    "        x=alt.X(\n",
    "            \"method:N\",\n",
    "            title=None,\n",
    "            sort=ordered_methods,\n",
    "            axis=alt.Axis(labels=False, ticks=False),\n",
    "        ),\n",
    "    )\n",
    "    if scale is not None:\n",
    "        y_scale = alt.Scale(domain=scale, clamp=True)\n",
    "    else:\n",
    "        y_scale = alt.Scale()\n",
    "\n",
    "    layered = (\n",
    "        base.mark_bar().encode(\n",
    "            y=alt.Y(\n",
    "                \"iou:Q\",\n",
    "                title=\"Mean IoU\",\n",
    "                scale=y_scale,\n",
    "            ),\n",
    "            color=alt.Color(\n",
    "                \"method:N\",\n",
    "                scale=color_scale,\n",
    "                title=\"Method\",\n",
    "                legend=alt.Legend(\n",
    "                    orient=\"bottom\",\n",
    "                    direction=\"horizontal\",\n",
    "                    titleAnchor=\"start\",\n",
    "                    columns=4,\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        + base.mark_errorbar(\n",
    "            extent=\"ci\", thickness=2.0, ticks=True, color=\"black\"\n",
    "        ).encode(\n",
    "            y=alt.Y(\n",
    "                \"iou_low:Q\",\n",
    "                title=None,\n",
    "                scale=y_scale,\n",
    "            ),\n",
    "            y2=\"iou_high:Q\",\n",
    "        )\n",
    "        + base.mark_text(align=\"center\", baseline=\"top\", dy=85, fontSize=14).encode(\n",
    "            text=alt.Text(\"iou:Q\", format=\".0f\"),\n",
    "        )\n",
    "    ).properties(width=200, height=200)  # type: ignore\n",
    "\n",
    "    chart = (\n",
    "        layered.facet(\n",
    "            row=alt.Row(\"dataset:N\", title=\"Dataset\"),\n",
    "            column=alt.Column(\"iou_type:N\", title=\"IoU (%)\", sort=\"descending\"),\n",
    "        )\n",
    "        .configure_axis(labelFontSize=12, titleFontSize=16)\n",
    "        .configure_header(labelFontSize=12, titleFontSize=16)\n",
    "        .configure_legend(labelFontSize=14, titleFontSize=16)\n",
    "    )\n",
    "\n",
    "    if scale is None:\n",
    "        chart = chart.resolve_scale(y=\"independent\")\n",
    "\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compose_bar_chart(\n",
    "    [\n",
    "        (disc_comparison_df, \"Overall Mean - Optic Disc\"),\n",
    "        (best_disc_comparison_df, \"Best Mean - Optic Disc\"),\n",
    "    ],\n",
    "    (50, 100),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compose_bar_chart(\n",
    "    [\n",
    "        (cup_comparison_df, \"Overall Mean - Optic Cup\"),\n",
    "        (best_cup_comparison_df, \"Best Mean  - Optic Cup\"),\n",
    "    ],\n",
    "    (0, 80),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_line_chart(data: pd.DataFrame):\n",
    "    new_data = data.copy()\n",
    "    new_data[\"shot\"] = new_data[\"shot\"].apply(\n",
    "        lambda x: f\"{x} shot\" if x == 1 else f\"{x} shots\"\n",
    "    )\n",
    "    new_data[\"sparsity_mode\"] = \"IoU \" + new_data[\"sparsity_mode\"] + \" (%)\"\n",
    "    encodings = {\n",
    "        \"x\": alt.X(\"sparsity_value\", title=None),\n",
    "        \"y\": alt.Y(\"mean(iou)\", title=None),\n",
    "        \"color\": alt.Color(\n",
    "            \"method\",\n",
    "            title=\"Methods\",\n",
    "            scale=color_scale,\n",
    "            legend=alt.Legend(\n",
    "                orient=\"bottom\", direction=\"horizontal\", titleAnchor=\"start\"\n",
    "            ),\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    error_band = (\n",
    "        alt.Chart(new_data).mark_errorband(extent=\"ci\", opacity=0.5).encode(**encodings)\n",
    "    )\n",
    "    line = alt.Chart(new_data).mark_line(strokeWidth=1).encode(**encodings)\n",
    "    point = alt.Chart(new_data).mark_point(size=5).encode(**encodings)\n",
    "\n",
    "    combined_chart = error_band + line + point  # type: ignore\n",
    "    combined_chart = (\n",
    "        combined_chart.properties(width=150, height=150)\n",
    "        .facet(\n",
    "            row=alt.Row(\"sparsity_mode\", title=None),\n",
    "            column=alt.Column(\n",
    "                \"shot\",\n",
    "                sort=[\"1-shot\", \"5-shot\", \"10-shot\", \"15-shot\", \"20-shot\"],\n",
    "                header=alt.Header(title=\"Sparsity Values\", titleOrient=\"bottom\"),\n",
    "            ),\n",
    "            spacing=10,\n",
    "        )\n",
    "        .resolve_scale(x=\"independent\")\n",
    "        .configure_axis(labelFontSize=12)\n",
    "        .configure_header(labelFontSize=16, titleFontSize=16)\n",
    "        .configure_legend(labelFontSize=14, titleFontSize=16)\n",
    "    )\n",
    "\n",
    "    return combined_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compose_line_chart(meta_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_metrics_df_with_ref = meta_metrics_df.copy()\n",
    "\n",
    "# meta_metrics_df_with_ref[\"method_parent\"] = (\n",
    "#     meta_metrics_df_with_ref[\"method\"].str.split(\"-\").str[-1]\n",
    "# )\n",
    "# meta_metrics_df_with_ref[\"method_child\"] = meta_metrics_df_with_ref[\"method\"].apply(\n",
    "#     lambda x: x.split(\"-\")[0] if \"-\" in x else \"original\"\n",
    "# )\n",
    "\n",
    "# meta_metrics_df_with_ref = pd.merge(\n",
    "#     meta_metrics_df_with_ref,\n",
    "#     simple_metrics_df.groupby([\"dataset\"])[\"iou\"].mean(),\n",
    "#     on=\"dataset\",\n",
    "#     suffixes=(\"\", \"_ref\"),\n",
    "# )\n",
    "\n",
    "# meta_metrics_df_with_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base = alt.Chart(meta_metrics_df_with_ref)\n",
    "\n",
    "# lines = base.mark_line().encode(\n",
    "#     x=alt.X(\"sparsity_value\", title=None),\n",
    "#     y=\"mean(iou)\",\n",
    "#     color=alt.Color(\n",
    "#         \"method_parent\",\n",
    "#         scale=alt.Scale(domain=[\"ProtoSeg\", \"WeaSeL\"], range=[\"#ff4444\", \"#77aaff\"]),\n",
    "#     ),\n",
    "#     strokeDash=\"method_child\",\n",
    "# )\n",
    "\n",
    "# ref_lines = base.mark_rule(color=\"#33cc33\").encode(y=\"mean(iou_ref)\")\n",
    "\n",
    "# (lines + ref_lines).properties(width=150, height=150).facet(\n",
    "#     row=\"sparsity_mode\", column=\"shot\"\n",
    "# ).resolve_scale(x=\"independent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = meta_metrics_df[\n",
    "#     (meta_metrics_df[\"method\"].str.endswith(\"ProtoSeg\"))\n",
    "#     & (meta_metrics_df[\"dataset\"] == \"REFUGE\")\n",
    "# ]\n",
    "# alt.Chart(data).mark_errorband(extent=\"ci\").encode(\n",
    "#     x=\"sparsity_value\",\n",
    "#     y=\"mean(iou)\",\n",
    "#     color=\"method\",\n",
    "# ).properties(width=300, height=200).facet(\n",
    "#     row=\"sparsity_mode\", column=\"shot\"\n",
    "# ).resolve_scale(\n",
    "#     x=\"independent\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = meta_metrics_df[\n",
    "#     (meta_metrics_df[\"method\"].str.endswith(\"WeaSeL\"))\n",
    "#     & (meta_metrics_df[\"dataset\"] == \"REFUGE\")\n",
    "# ]\n",
    "# alt.Chart(data).mark_errorband(extent=\"ci\").encode(\n",
    "#     x=\"sparsity_value\",\n",
    "#     y=\"mean(iou)\",\n",
    "#     color=\"method\",\n",
    "# ).properties(width=300, height=200).facet(\n",
    "#     row=\"sparsity_mode\", column=\"shot\"\n",
    "# ).resolve_scale(\n",
    "#     x=\"independent\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results_df = pd.merge(\n",
    "    best_disc_comparison_df,\n",
    "    best_cup_comparison_df,\n",
    "    on=best_disc_comparison_df.columns.tolist()[:5],\n",
    "    suffixes=(\"_disc\", \"_cup\"),\n",
    ")\n",
    "best_results_df[\"method_order\"] = best_results_df[\"method\"].map(\n",
    "    {\n",
    "        \"ProtoSeg\": 0,\n",
    "        \"O-ProtoSeg\": 1,\n",
    "        \"EO-ProtoSeg\": 2,\n",
    "        \"SL\": 3,\n",
    "        \"WeaSeL\": 4,\n",
    "        \"O-WeaSeL\": 5,\n",
    "        \"EO-WeaSeL\": 6,\n",
    "    }\n",
    ")\n",
    "best_results_df.sort_values([\"dataset\", \"method_order\"], inplace=True)\n",
    "\n",
    "best_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(best_results_df)):\n",
    "    row = best_results_df.iloc[i]\n",
    "    is_sl = row[\"method\"] == \"SL\"\n",
    "    is_point = row[\"sparsity_mode\"] == \"point\"\n",
    "    if is_sl:\n",
    "        shot, sparsity = \"-\", \"-\"\n",
    "    elif is_point:\n",
    "        shot, sparsity = int(row[\"shot\"]), \"point - \" + str(int(row[\"sparsity_value\"]))\n",
    "    else:\n",
    "        shot, sparsity = (\n",
    "            int(row[\"shot\"]),\n",
    "            row[\"sparsity_mode\"] + \" - \" + f\"{row['sparsity_value']:.2f}\",\n",
    "        )\n",
    "    print(\n",
    "        f\"& {row['method']} & {shot} & {sparsity} & {row['iou_disc']:.2f} & {row['iou_low_disc']:.2f}-{row['iou_high_disc']:.2f} & {row['iou_cup']:.2f} & {row['iou_low_cup']:.2f}-{row['iou_high_cup']:.2f} \\\\\\\\\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eop_metrics_df = meta_metrics_df[(meta_metrics_df[\"method\"] == \"EO-ProtoSeg\")]\n",
    "\n",
    "agg_eop_metrics_df = (\n",
    "    eop_metrics_df.groupby([\"sparsity_mode\", \"sparsity_value\", \"shot\"])\n",
    "    .agg(iou=(\"iou\", \"mean\"))\n",
    "    .sort_values([\"sparsity_mode\", \"sparsity_value\", \"shot\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg_eop_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_eop_metrics_ls = [[[] for _ in range(5)] for _ in range(5)]\n",
    "\n",
    "for i in range(len(agg_eop_metrics_df)):\n",
    "    row = agg_eop_metrics_df.iloc[i]\n",
    "    ls = agg_eop_metrics_ls[i // 25][(i % 25) // 5]\n",
    "    if len(ls) == 0:\n",
    "        ls.append(row[\"sparsity_value\"])\n",
    "    ls.append(row[\"iou\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_modes = [\"contours\", \"grid\", \"point\", \"regions\", \"skeleton\"]\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"\\\\multirow{5}{*}{mode}  \".replace(\"mode\", sparsity_modes[i]), end=\"\")\n",
    "    print(\n",
    "        \" \\\\\\\\\\n\".join(\n",
    "            [\n",
    "                \"& \" + \" & \".join([f\"{np.mean(v):.2f}\" for v in ls])\n",
    "                for ls in agg_eop_metrics_ls[i]\n",
    "            ]\n",
    "        )\n",
    "        + \" \\\\\\\\\"\n",
    "    )\n",
    "    print(\"\\\\hline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_two_methods_score_od_oc(\n",
    "    higher_method: str, lower_method: str\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = meta_metrics_df.drop(\n",
    "        columns=[\"loss\", \"iou\", \"shot\", \"sparsity_mode\", \"sparsity_value\"]\n",
    "    )\n",
    "    higher_df = df[df[\"method\"] == higher_method]\n",
    "    lower_df = df[df[\"method\"] == lower_method]\n",
    "\n",
    "    merged_df = pd.merge(\n",
    "        higher_df, lower_df, on=[\"dataset\", \"batch\"], suffixes=(\"_h\", \"_l\")\n",
    "    )\n",
    "    n = len(merged_df)\n",
    "\n",
    "    merged_df[\"iou_cup_diff\"] = merged_df[\"iou_cup_h\"] - merged_df[\"iou_cup_l\"]\n",
    "    merged_df[\"iou_disc_diff\"] = merged_df[\"iou_disc_h\"] - merged_df[\"iou_disc_l\"]\n",
    "\n",
    "    grouped_df = merged_df.groupby([\"dataset\"])[[\"iou_cup_diff\", \"iou_disc_diff\"]]\n",
    "    t_vals = grouped_df.mean() / (grouped_df.std() / np.sqrt(n))\n",
    "\n",
    "    p_vals = t_vals.apply(lambda x: stats.t.sf(x, n - 1))\n",
    "\n",
    "    return t_vals, p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_two_methods_score(higher_method: str, lower_method: str):\n",
    "#     df = meta_metrics_df.drop(\n",
    "#         columns=[\n",
    "#             \"loss\",\n",
    "#             \"iou_cup\",\n",
    "#             \"iou_disc\",\n",
    "#             \"shot\",\n",
    "#             \"sparsity_mode\",\n",
    "#             \"sparsity_value\",\n",
    "#         ]\n",
    "#     )\n",
    "#     higher_df = df[df[\"method\"] == higher_method]\n",
    "#     lower_df = df[df[\"method\"] == lower_method]\n",
    "#     merged_df = pd.merge(\n",
    "#         higher_df, lower_df, on=[\"dataset\", \"batch\"], suffixes=(\"_h\", \"_l\")\n",
    "#     )\n",
    "\n",
    "#     grouped_df = merged_df.groupby([\"dataset\"])[[\"iou_h\", \"iou_l\"]]\n",
    "#     return grouped_df.apply(lambda x: stats.wilcoxon(x[\"iou_h\"], x[\"iou_l\"], alternative=\"greater\"))\n",
    "\n",
    "# merged_df[\"iou_diff\"] = merged_df[\"iou_h\"] - merged_df[\"iou_l\"]\n",
    "# grouped_df = merged_df.groupby([\"dataset\"])[\"iou_diff\"]\n",
    "# n = len(merged_df)\n",
    "# t_vals = grouped_df.mean() / (grouped_df.std() / np.sqrt(n))\n",
    "# p_vals = t_vals.apply(lambda x: stats.t.sf(x, n - 1))\n",
    "# return t_vals, p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_two_methods_score(higher_method: str, lower_method: str):\n",
    "    best_indices = (\n",
    "        meta_metrics_df.groupby(\n",
    "            [\"dataset\", \"method\", \"shot\", \"sparsity_mode\", \"sparsity_value\"],\n",
    "            dropna=False,\n",
    "        )[\"iou\"]\n",
    "        .mean()\n",
    "        .groupby([\"dataset\", \"method\"])\n",
    "        .idxmax()\n",
    "    )\n",
    "\n",
    "    def get_best_ious(method: str) -> pd.DataFrame:\n",
    "        df_list = []\n",
    "        for ds in meta_metrics_df[\"dataset\"].unique():\n",
    "            _, _, shot, sparsity_mode, sparsity_value = best_indices[ds, method]\n",
    "            df = meta_metrics_df[\n",
    "                (meta_metrics_df[\"dataset\"] == ds)\n",
    "                & (meta_metrics_df[\"method\"] == method)\n",
    "                & (meta_metrics_df[\"shot\"] == shot)\n",
    "                & (meta_metrics_df[\"sparsity_mode\"] == sparsity_mode)\n",
    "                & (meta_metrics_df[\"sparsity_value\"] == sparsity_value)\n",
    "            ]\n",
    "            min_batch = df[\"batch\"].min()\n",
    "            df.loc[:, \"batch\"] = df[\"batch\"] - min_batch + 1\n",
    "            df_list.append(df[[\"dataset\", \"method\", \"batch\", \"iou\"]])\n",
    "        return pd.concat(df_list).reset_index(drop=True)\n",
    "\n",
    "    higher_df = get_best_ious(higher_method)\n",
    "    lower_df = get_best_ious(lower_method)\n",
    "    merged_df = pd.merge(\n",
    "        higher_df, lower_df, on=[\"dataset\", \"batch\"], suffixes=(\"_h\", \"_l\")\n",
    "    )\n",
    "\n",
    "    grouped_df = merged_df.groupby([\"dataset\"])[[\"iou_h\", \"iou_l\"]]\n",
    "    return grouped_df.apply(\n",
    "        lambda x: stats.wilcoxon(x[\"iou_h\"], x[\"iou_l\"], alternative=\"greater\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_two_methods_score(\"EO-ProtoSeg\", \"ProtoSeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_two_methods_score(\"EO-WeaSeL\", \"WeaSeL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_two_methods_score(\"EO-ProtoSeg\", \"EO-WeaSeL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_two_methods_score(\"ProtoSeg\", \"WeaSeL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff_value = 5\n",
    "# dropped_columns = [\n",
    "#     \"shot\",\n",
    "#     \"sparsity_mode\",\n",
    "#     \"sparsity_value\",\n",
    "#     \"iou_std_err\",\n",
    "#     \"iou_low\",\n",
    "#     \"iou_high\",\n",
    "# ]\n",
    "# rename_columns = {\n",
    "#     \"iou_ref\": \"ref\",\n",
    "#     \"iou\": \"mean\",\n",
    "#     \"iou_std\": \"std\",\n",
    "#     \"iou_count\": \"n\",\n",
    "# }\n",
    "# disc_df = best_disc_comparison_df.drop(columns=dropped_columns).rename(\n",
    "#     columns=rename_columns\n",
    "# )\n",
    "# cup_df = best_cup_comparison_df.drop(columns=dropped_columns).rename(\n",
    "#     columns=rename_columns\n",
    "# )\n",
    "\n",
    "# simple_disc_df = disc_df[disc_df[\"method\"] == \"SL\"].drop(columns=[\"method\", \"ref\"])\n",
    "# simple_disc_df[\"object\"] = \"disc\"\n",
    "# simple_cup_df = cup_df[cup_df[\"method\"] == \"SL\"].drop(columns=[\"method\", \"ref\"])\n",
    "# simple_cup_df[\"object\"] = \"cup\"\n",
    "# simple_df = pd.concat([simple_disc_df, simple_cup_df], axis=0)\n",
    "\n",
    "# meta_disc_df = disc_df.loc[\n",
    "#     disc_df[disc_df[\"method\"] != \"SL\"].groupby([\"dataset\"])[\"ref\"].idxmax()\n",
    "# ].drop(columns=[\"method\", \"ref\"])\n",
    "# meta_disc_df[\"object\"] = \"disc\"\n",
    "# meta_cup_df = cup_df.loc[\n",
    "#     cup_df[cup_df[\"method\"] != \"SL\"].groupby([\"dataset\"])[\"ref\"].idxmax()\n",
    "# ].drop(columns=[\"method\", \"ref\"])\n",
    "# meta_cup_df[\"object\"] = \"cup\"\n",
    "# meta_df = pd.concat([meta_disc_df, meta_cup_df], axis=0)\n",
    "\n",
    "# df = pd.merge(\n",
    "#     simple_df,\n",
    "#     meta_df,\n",
    "#     on=[\"dataset\", \"object\"],\n",
    "#     suffixes=(\"_simple\", \"_meta\"),\n",
    "# )\n",
    "\n",
    "# var_simple = (df[\"std_simple\"] ** 2) / df[\"n_simple\"]\n",
    "# var_meta = (df[\"std_meta\"] ** 2) / df[\"n_meta\"]\n",
    "\n",
    "# df[\"t_value\"] = (df[\"mean_simple\"] - df[\"mean_meta\"] - diff_value) / (\n",
    "#     var_simple + var_meta\n",
    "# ) ** 0.5\n",
    "\n",
    "# df[\"dof\"] = ((var_simple + var_meta) ** 2) / (\n",
    "#     (var_simple**2) / (df[\"n_simple\"] - 1) + (var_meta**2) / (df[\"n_meta\"] - 1)\n",
    "# )\n",
    "\n",
    "# df[\"p_value\"] = stats.t.cdf(df[\"t_value\"], df[\"dof\"]).round(6)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_meta_df = (\n",
    "    meta_metrics_df.groupby(\n",
    "        [\"dataset\", \"method\", \"shot\", \"sparsity_mode\", \"sparsity_value\"],\n",
    "        dropna=False,\n",
    "    )\n",
    "    .agg(\n",
    "        iou=(\"iou\", \"mean\"),\n",
    "        iou_cup=(\"iou_cup\", \"mean\"),\n",
    "        iou_disc=(\"iou_disc\", \"mean\"),\n",
    "    )\n",
    "    .groupby([\"dataset\"])[\"iou\"]\n",
    "    .idxmax()\n",
    ")\n",
    "\n",
    "diff_value = 0\n",
    "stat_test_result = []\n",
    "for dataset in [\"DRISHTI-GS\", \"REFUGE\", \"RIM-ONE-3\"]:\n",
    "    for object in [\"disc\", \"cup\"]:\n",
    "        simple_score = simple_metrics_df[(simple_metrics_df[\"dataset\"] == dataset)][\n",
    "            f\"iou_{object}\"\n",
    "        ]\n",
    "\n",
    "        _, method, shot, sparsity_mode, sparsity_value = best_meta_df.loc[dataset]\n",
    "        meta_score = meta_metrics_df[\n",
    "            (meta_metrics_df[\"dataset\"] == dataset)\n",
    "            & (meta_metrics_df[\"method\"] == method)\n",
    "            & (meta_metrics_df[\"shot\"] == shot)\n",
    "            & (meta_metrics_df[\"sparsity_mode\"] == sparsity_mode)\n",
    "            & (meta_metrics_df[\"sparsity_value\"] == sparsity_value)\n",
    "        ][f\"iou_{object}\"]\n",
    "\n",
    "        statistic, pvalue = stats.mannwhitneyu(\n",
    "            simple_score - diff_value, meta_score, alternative=\"greater\"\n",
    "        )\n",
    "\n",
    "        stat_test_result.append(\n",
    "            {\n",
    "                \"dataset\": dataset,\n",
    "                \"object\": object,\n",
    "                \"simple_score\": simple_score.mean(),\n",
    "                \"meta_score\": meta_score.mean(),\n",
    "                \"meta_method\": method,\n",
    "                \"meta_shot\": shot,\n",
    "                \"meta_sparsity_mode\": sparsity_mode,\n",
    "                \"meta_sparsity_value\": sparsity_value,\n",
    "                \"statistic\": statistic,\n",
    "                \"p_value\": pvalue,\n",
    "            }\n",
    "        )\n",
    "\n",
    "pd.DataFrame(stat_test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Profiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# from utils.wandb import wandb_path\n",
    "\n",
    "# runs = wandb.Api().runs(\n",
    "#     wandb_path(False),\n",
    "#     filters={\"jobType\": \"profile-test\", \"createdAt\": {\"$gt\": \"2025-01-01T00:00:00Z\"}},\n",
    "# )\n",
    "\n",
    "# group_names = {\n",
    "#     \"SL\": \"SL\",\n",
    "#     \"WS-ori\": \"WeaSeL\",\n",
    "#     \"WS-ms\": \"O-WeaSeL\",\n",
    "#     \"WS\": \"EO-WeaSeL\",\n",
    "#     \"PS-ori\": \"ProtoSeg\",\n",
    "#     \"PS-mp\": \"O-ProtoSeg\",\n",
    "#     \"PS\": \"EO-ProtoSeg\",\n",
    "# }\n",
    "\n",
    "# for i, run in enumerate(runs):\n",
    "#     group = group_names[run.group]\n",
    "#     run_id = run.name.split(\" \")[-1]\n",
    "#     batch_size = run.config[\"batch_size\"]\n",
    "#     shot = run.config.get(\"shot\", -1)\n",
    "#     shot_str = f\" s{shot}\" if shot != -1 else \"\"\n",
    "#     run.logged_artifacts()[0].download(\n",
    "#         f\"logs/wandb/1_test_profile/{group} b{batch_size}{shot_str} {run_id}\"\n",
    "#     )\n",
    "#     print(group, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# df_list = []\n",
    "\n",
    "# wandb_dir = \"logs/wandb/1_test_profile\"\n",
    "# for i, dir in enumerate(os.listdir(wandb_dir)):\n",
    "#     if os.path.isfile(f\"{wandb_dir}/{dir}\"):\n",
    "#         continue\n",
    "#     splitted = dir.split(\" \")\n",
    "#     if len(splitted) == 3:\n",
    "#         group, batch_str, _ = splitted\n",
    "#         shot = -1\n",
    "#     else:\n",
    "#         group, batch_str, shot_str, _ = splitted\n",
    "#         shot = int(shot_str[1:])\n",
    "#     batch_size = int(batch_str[1:])\n",
    "#     df = read_wandb_table(f\"{wandb_dir}/{dir}/test_profile.table.json\")\n",
    "#     df.insert(0, \"shot\", shot)\n",
    "#     df.insert(0, \"batch_size\", batch_size)\n",
    "#     df.insert(0, \"method\", group)\n",
    "#     df.insert(0, \"index\", i)\n",
    "#     df_list.append(df)\n",
    "\n",
    "# test_profile_df = pd.concat(df_list)\n",
    "# test_profile_df.to_csv(\"logs/wandb/1_test_profile.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_confidence_limits(\n",
    "    data: pd.DataFrame,\n",
    "    mean_col: str = \"Mean (s)\",\n",
    "    std_col: str = \"Std (s)\",\n",
    "    ci: Literal[90, 95, 99] = 95,\n",
    ") -> pd.DataFrame:\n",
    "    data[\"Std Err\"] = data[std_col] / (data[\"Num Calls\"]) ** 0.5\n",
    "    if ci == 90:\n",
    "        z = 1.645\n",
    "    elif ci == 95:\n",
    "        z = 1.96\n",
    "    elif ci == 99:\n",
    "        z = 2.576\n",
    "    data[f\"CL {ci} L\"] = data[mean_col] - z * data[\"Std Err\"]\n",
    "    data[f\"CL {ci} U\"] = data[mean_col] + z * data[\"Std Err\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_mean(data: pd.DataFrame, mean_col: str, num_items_col: str) -> float:\n",
    "    return (data[mean_col] * data[num_items_col]).sum() / data[num_items_col].sum()\n",
    "\n",
    "\n",
    "def combine_variance(\n",
    "    data: pd.DataFrame,\n",
    "    mean_col: str,\n",
    "    var_col: str,\n",
    "    num_items_col: str,\n",
    "    combined_mean: float | None = None,\n",
    ") -> float:\n",
    "    num_items = data[num_items_col]\n",
    "    variances = data[var_col] ** 2\n",
    "    means = data[mean_col]\n",
    "    if combined_mean is None:\n",
    "        combined_mean = combine_mean(data, mean_col, num_items_col)\n",
    "\n",
    "    weighted_var = ((num_items - 1) * variances).sum()\n",
    "    between_var = (num_items * (means - combined_mean) ** 2).sum()\n",
    "\n",
    "    total_num_items = num_items.sum()\n",
    "    return (weighted_var + between_var) / total_num_items\n",
    "\n",
    "\n",
    "def combine_mean_variance(\n",
    "    data: pd.DataFrame,\n",
    "    groupby_cols: list[str],\n",
    "    mean_col: str,\n",
    "    std_col: str,\n",
    "    num_items_col: str,\n",
    ") -> pd.DataFrame:\n",
    "    def agg_func(data: pd.DataFrame) -> pd.Series:\n",
    "        combined_mean = combine_mean(data, mean_col, num_items_col)\n",
    "        combined_std = (\n",
    "            combine_variance(data, mean_col, std_col, num_items_col, combined_mean)\n",
    "            ** 0.5\n",
    "        )\n",
    "        total_num_items = data[num_items_col].sum()\n",
    "        return pd.Series(\n",
    "            {\n",
    "                mean_col: combined_mean,\n",
    "                std_col: combined_std,\n",
    "                num_items_col: total_num_items,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    combined_df = data.groupby(groupby_cols).apply(agg_func).reset_index()\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_profile_df = pd.read_csv(\"logs/wandb/test_profile.csv\")\n",
    "\n",
    "test_profile_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Learner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_test_profile_df = test_profile_df[test_profile_df[\"method\"].isin([\"SL\"])]\n",
    "\n",
    "sl_inf_df = sl_test_profile_df[\n",
    "    sl_test_profile_df[\"Action\"] == \"[Learner]SimpleUnet.forward\"\n",
    "].drop(columns=[\"index\", \"Action\", \"Percentage (%)\"])\n",
    "sl_inf_df[\"Mean per Image (s)\"] = sl_inf_df[\"Mean (s)\"] / sl_inf_df[\"batch_size\"]\n",
    "sl_inf_df[\"Std per Image (s)\"] = sl_inf_df[\"Std (s)\"] / sl_inf_df[\"batch_size\"]\n",
    "\n",
    "sl_inf_df = calc_confidence_limits(sl_inf_df, \"Mean per Image (s)\", \"Std per Image (s)\")\n",
    "\n",
    "line_chart = (\n",
    "    alt.Chart(sl_inf_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=\"Mean per Image (s)\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "error_chart = (\n",
    "    alt.Chart(sl_inf_df)\n",
    "    .mark_errorband()\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=alt.Y(\"CL 95 U\").title(\"Time per Image CL 95 (s)\"),\n",
    "        y2=\"CL 95 L\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "(line_chart + error_chart).properties(width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProtoSeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_test_profile_df = test_profile_df[test_profile_df[\"method\"].str.endswith(\"ProtoSeg\")]\n",
    "\n",
    "ps_test_profile_df[ps_test_profile_df[\"index\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_inf_df = calc_confidence_limits(\n",
    "    ps_test_profile_df[\n",
    "        ps_test_profile_df[\"Action\"] == \"[Learner]ProtosegUnet.get_prototypes\"\n",
    "    ].drop(columns=[\"index\", \"Action\", \"Percentage (%)\"]),\n",
    ")\n",
    "\n",
    "line_chart = (\n",
    "    alt.Chart(ps_inf_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=\"Mean (s)\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "error_chart = (\n",
    "    alt.Chart(ps_inf_df)\n",
    "    .mark_errorband()\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=alt.Y(\"CL 95 U\").title(\"Time CL 95 (s)\"),\n",
    "        y2=\"CL 95 L\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "(line_chart + error_chart).properties(width=300, height=200).facet(column=\"shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_inf_df = ps_test_profile_df[\n",
    "    ps_test_profile_df[\"Action\"] == \"[Learner]ProtosegUnet.prediction\"\n",
    "].drop(columns=[\"index\", \"Action\", \"Percentage (%)\"])\n",
    "ps_inf_df[\"Mean per Image (s)\"] = ps_inf_df[\"Mean (s)\"] / ps_inf_df[\"batch_size\"]\n",
    "ps_inf_df[\"Std per Image (s)\"] = ps_inf_df[\"Std (s)\"] / ps_inf_df[\"batch_size\"]\n",
    "\n",
    "ps_inf_df = combine_mean_variance(\n",
    "    ps_inf_df,\n",
    "    groupby_cols=[\"method\", \"batch_size\"],\n",
    "    mean_col=\"Mean per Image (s)\",\n",
    "    std_col=\"Std per Image (s)\",\n",
    "    num_items_col=\"Num Calls\",\n",
    ")\n",
    "\n",
    "ps_inf_df = calc_confidence_limits(ps_inf_df, \"Mean per Image (s)\", \"Std per Image (s)\")\n",
    "\n",
    "ps_sl_inf_df = pd.concat(\n",
    "    [\n",
    "        sl_inf_df[sl_inf_df[\"batch_size\"] <= 16].drop(\n",
    "            columns=[\"shot\", \"Mean (s)\", \"Std (s)\", \"Sum (s)\"]\n",
    "        ),\n",
    "        ps_inf_df,\n",
    "    ]\n",
    ")\n",
    "\n",
    "ps_sl_inf_df = ps_sl_inf_df[ps_sl_inf_df[\"batch_size\"] >= 2]\n",
    "\n",
    "line_chart = (\n",
    "    alt.Chart(ps_sl_inf_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=\"Mean per Image (s)\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "error_chart = (\n",
    "    alt.Chart(ps_sl_inf_df)\n",
    "    .mark_errorband(opacity=0.2)\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=alt.Y(\"CL 95 U\").title(\"Time per Image CL 95 (s)\"),\n",
    "        y2=\"CL 95 L\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "(line_chart + error_chart).properties(width=600, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_inf_df = ps_test_profile_df[\n",
    "    ps_test_profile_df[\"Action\"] == \"[Learner]ProtosegUnet.post_process\"\n",
    "].drop(columns=[\"index\", \"Action\", \"Percentage (%)\"])\n",
    "ps_inf_df[\"Mean per Image (s)\"] = ps_inf_df[\"Mean (s)\"] / ps_inf_df[\"batch_size\"]\n",
    "ps_inf_df[\"Std per Image (s)\"] = ps_inf_df[\"Std (s)\"] / ps_inf_df[\"batch_size\"]\n",
    "\n",
    "ps_inf_df = combine_mean_variance(\n",
    "    ps_inf_df,\n",
    "    groupby_cols=[\"method\", \"batch_size\"],\n",
    "    mean_col=\"Mean per Image (s)\",\n",
    "    std_col=\"Std per Image (s)\",\n",
    "    num_items_col=\"Num Calls\",\n",
    ")\n",
    "\n",
    "ps_inf_df = calc_confidence_limits(ps_inf_df, \"Mean per Image (s)\", \"Std per Image (s)\")\n",
    "\n",
    "ps_inf_df = ps_inf_df[ps_inf_df[\"batch_size\"] >= 2]\n",
    "\n",
    "line_chart = (\n",
    "    alt.Chart(ps_inf_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=\"Mean per Image (s)\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "error_chart = (\n",
    "    alt.Chart(ps_inf_df)\n",
    "    .mark_errorband(opacity=0.2)\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=alt.Y(\"CL 95 U\").title(\"Time per Image CL 95 (s)\"),\n",
    "        y2=\"CL 95 L\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "(line_chart + error_chart).properties(width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WeaSeL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_test_profile_df = test_profile_df[test_profile_df[\"method\"].str.endswith(\"WeaSeL\")]\n",
    "\n",
    "tp_rows = ws_test_profile_df[\"Action\"] == \"[Learner]WeaselUnet.tune_process\"\n",
    "\n",
    "ws_test_profile_df.loc[tp_rows, \"Num Calls\"] = (\n",
    "    ws_test_profile_df.loc[tp_rows, \"Num Calls\"] // 33\n",
    ")\n",
    "ws_test_profile_df.loc[tp_rows, \"Mean (s)\"] = (\n",
    "    ws_test_profile_df.loc[tp_rows, \"Mean (s)\"] * 33\n",
    ")\n",
    "ws_test_profile_df.loc[tp_rows, \"Std (s)\"] = (\n",
    "    ws_test_profile_df.loc[tp_rows, \"Std (s)\"] * 33**0.5\n",
    ")\n",
    "\n",
    "# ws_test_profile_df[ws_test_profile_df[\"index\"] == 80]\n",
    "ws_test_profile_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_inf_df = calc_confidence_limits(\n",
    "    ws_test_profile_df[\n",
    "        ws_test_profile_df[\"Action\"] == \"[Learner]WeaselUnet.tune_process\"\n",
    "    ].drop(columns=[\"index\", \"Action\", \"Percentage (%)\"]),\n",
    ")\n",
    "\n",
    "line_chart = (\n",
    "    alt.Chart(ws_inf_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=\"Mean (s)\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "error_chart = (\n",
    "    alt.Chart(ws_inf_df)\n",
    "    .mark_errorband()\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=alt.Y(\"CL 95 U\").title(\"Time CL 95 (s)\"),\n",
    "        y2=\"CL 95 L\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "(line_chart + error_chart).properties(width=300, height=200).facet(column=\"shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_inf_df = ws_test_profile_df[\n",
    "    ws_test_profile_df[\"Action\"] == \"[Learner]WeaselUnet.prediction\"\n",
    "].drop(columns=[\"index\", \"Action\", \"Percentage (%)\"])\n",
    "ws_inf_df[\"Mean per Image (s)\"] = ws_inf_df[\"Mean (s)\"] / ws_inf_df[\"batch_size\"]\n",
    "ws_inf_df[\"Std per Image (s)\"] = ws_inf_df[\"Std (s)\"] / ws_inf_df[\"batch_size\"]\n",
    "\n",
    "ws_inf_df = combine_mean_variance(\n",
    "    ws_inf_df,\n",
    "    groupby_cols=[\"method\", \"batch_size\"],\n",
    "    mean_col=\"Mean per Image (s)\",\n",
    "    std_col=\"Std per Image (s)\",\n",
    "    num_items_col=\"Num Calls\",\n",
    ")\n",
    "\n",
    "ws_inf_df = calc_confidence_limits(ws_inf_df, \"Mean per Image (s)\", \"Std per Image (s)\")\n",
    "ws_inf_df[\"CL 95 L\"] = ws_inf_df[\"CL 95 L\"].clip(lower=0)\n",
    "\n",
    "ws_sl_inf_df = pd.concat(\n",
    "    [\n",
    "        sl_inf_df[sl_inf_df[\"batch_size\"] <= 16].drop(\n",
    "            columns=[\"shot\", \"Mean (s)\", \"Std (s)\", \"Sum (s)\"]\n",
    "        ),\n",
    "        ws_inf_df,\n",
    "    ]\n",
    ")\n",
    "\n",
    "ws_sl_inf_df = ws_sl_inf_df[ws_sl_inf_df[\"batch_size\"] >= 2]\n",
    "\n",
    "line_chart = (\n",
    "    alt.Chart(ws_sl_inf_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=\"Mean per Image (s)\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "error_chart = (\n",
    "    alt.Chart(ws_sl_inf_df)\n",
    "    .mark_errorband(opacity=0.2)\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=alt.Y(\"CL 95 U\").title(\"Time per Image CL 95 (s)\"),\n",
    "        y2=\"CL 95 L\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "(line_chart + error_chart).properties(width=600, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_inf_df = ws_test_profile_df[\n",
    "    ws_test_profile_df[\"Action\"] == \"[Learner]WeaselUnet.post_process\"\n",
    "].drop(columns=[\"index\", \"Action\", \"Percentage (%)\"])\n",
    "ws_inf_df[\"Mean per Image (s)\"] = ws_inf_df[\"Mean (s)\"] / ws_inf_df[\"batch_size\"]\n",
    "ws_inf_df[\"Std per Image (s)\"] = ws_inf_df[\"Std (s)\"] / ws_inf_df[\"batch_size\"]\n",
    "\n",
    "ws_inf_df = combine_mean_variance(\n",
    "    ws_inf_df,\n",
    "    groupby_cols=[\"method\", \"batch_size\"],\n",
    "    mean_col=\"Mean per Image (s)\",\n",
    "    std_col=\"Std per Image (s)\",\n",
    "    num_items_col=\"Num Calls\",\n",
    ")\n",
    "\n",
    "ws_inf_df = calc_confidence_limits(ws_inf_df, \"Mean per Image (s)\", \"Std per Image (s)\")\n",
    "ws_inf_df[\"CL 95 L\"] = ws_inf_df[\"CL 95 L\"].clip(lower=0)\n",
    "\n",
    "ws_inf_df = ws_inf_df[ws_inf_df[\"batch_size\"] >= 2]\n",
    "\n",
    "line_chart = (\n",
    "    alt.Chart(ws_inf_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=\"Mean per Image (s)\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "error_chart = (\n",
    "    alt.Chart(ws_inf_df)\n",
    "    .mark_errorband(opacity=0.2)\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=alt.Y(\"CL 95 U\").title(\"Time per Image CL 95 (s)\"),\n",
    "        y2=\"CL 95 L\",\n",
    "        color=\"method\",\n",
    "    )\n",
    ")\n",
    "\n",
    "(line_chart + error_chart).properties(width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_methods = [\n",
    "    \"ProtoSeg\",\n",
    "    \"O-ProtoSeg\",\n",
    "    \"EO-ProtoSeg\",\n",
    "    \"SL\",\n",
    "    \"WeaSeL\",\n",
    "    \"O-WeaSeL\",\n",
    "    \"EO-WeaSeL\",\n",
    "]\n",
    "\n",
    "color_values = [\n",
    "    \"#ffda03\",  # Yellow\n",
    "    \"#e85d04\",  # Orange\n",
    "    \"#d00000\",  # Red\n",
    "    \"#757575\",  # Gray\n",
    "    \"#43b0f1\",  # Blue\n",
    "    \"#2ec4b6\",  # Turquoise\n",
    "    \"#2d6a4f\",  # Green\n",
    "]\n",
    "\n",
    "color_scale = alt.Scale(domain=ordered_methods, range=color_values)\n",
    "color_scale_no_sl = alt.Scale(\n",
    "    domain=ordered_methods[:3] + ordered_methods[4:],\n",
    "    range=color_values[:3] + color_values[4:],\n",
    ")\n",
    "color_scale_ps = alt.Scale(\n",
    "    domain=ordered_methods[:3],\n",
    "    range=color_values[:3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 150, 150\n",
    "\n",
    "color = alt.Color(\n",
    "    \"method:N\",\n",
    "    scale=color_scale_no_sl,\n",
    "    title=\"Method\",\n",
    "    legend=alt.Legend(\n",
    "        orient=\"bottom\",\n",
    "        direction=\"horizontal\",\n",
    "        titleAnchor=\"start\",\n",
    "        columns=6,\n",
    "        symbolOpacity=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "ps_inf_df = calc_confidence_limits(\n",
    "    ps_test_profile_df[\n",
    "        ps_test_profile_df[\"Action\"] == \"[Learner]ProtosegUnet.get_prototypes\"\n",
    "    ].drop(columns=[\"index\", \"Action\", \"Percentage (%)\"]),\n",
    ")\n",
    "ps_inf_df[\"shot\"] = ps_inf_df[\"shot\"].apply(lambda x: f\"{x}-shot\")\n",
    "\n",
    "line_chart = (\n",
    "    alt.Chart(ps_inf_df).mark_line().encode(x=\"batch_size\", y=\"Mean (s)\", color=color)\n",
    ")\n",
    "error_chart = (\n",
    "    alt.Chart(ps_inf_df)\n",
    "    .mark_errorband()\n",
    "    .encode(\n",
    "        x=alt.X(\"batch_size\")\n",
    "        .title(None)\n",
    "        .scale(domain=[1, 16], nice=False)\n",
    "        .axis(labels=False),\n",
    "        y=alt.Y(\"CL 95 U\").title(\"ProtoSeg Time (s)\").scale(nice=False),\n",
    "        y2=\"CL 95 L\",\n",
    "        color=color,\n",
    "    )\n",
    ")\n",
    "ps_chart = (\n",
    "    (line_chart + error_chart)\n",
    "    .properties(width=width, height=height)\n",
    "    .facet(\n",
    "        column=alt.Column(\n",
    "            \"shot\",\n",
    "            sort=[\"1-shot\", \"5-shot\", \"10-shot\", \"15-shot\", \"20-shot\"],\n",
    "            header=alt.Header(title=None),\n",
    "        ),\n",
    "        spacing=10,\n",
    "    )\n",
    ")\n",
    "\n",
    "ws_inf_df = calc_confidence_limits(\n",
    "    ws_test_profile_df[\n",
    "        ws_test_profile_df[\"Action\"] == \"[Learner]WeaselUnet.tune_process\"\n",
    "    ].drop(columns=[\"index\", \"Action\", \"Percentage (%)\"]),\n",
    ")\n",
    "ws_inf_df[\"shot\"] = ws_inf_df[\"shot\"].apply(lambda x: f\"{x}-shot\")\n",
    "\n",
    "line_chart = (\n",
    "    alt.Chart(ws_inf_df).mark_line().encode(x=\"batch_size\", y=\"Mean (s)\", color=color)\n",
    ")\n",
    "error_chart = (\n",
    "    alt.Chart(ws_inf_df)\n",
    "    .mark_errorband()\n",
    "    .encode(\n",
    "        x=alt.X(\"batch_size\").title(None).scale(domain=[1, 16], nice=False),\n",
    "        y=alt.Y(\"CL 95 U\").title(\"WeaSeL Time (s)\").scale(nice=False),\n",
    "        y2=\"CL 95 L\",\n",
    "        color=color,\n",
    "    )\n",
    ")\n",
    "ws_chart = (\n",
    "    (line_chart + error_chart)\n",
    "    .properties(width=width, height=height)\n",
    "    .facet(\n",
    "        column=alt.Column(\n",
    "            \"shot\",\n",
    "            sort=[\"1-shot\", \"5-shot\", \"10-shot\", \"15-shot\", \"20-shot\"],\n",
    "            header=alt.Header(title=\"Batch Size\", titleOrient=\"bottom\", labels=False),\n",
    "        ),\n",
    "        spacing=10,\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    alt.vconcat(ps_chart, ws_chart)\n",
    "    .configure_axis(labelFontSize=14, titleFontSize=14)\n",
    "    .configure_header(labelFontSize=14, titleFontSize=14)\n",
    "    .configure_legend(labelFontSize=14, titleFontSize=16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = alt.Color(\n",
    "    \"method:N\",\n",
    "    scale=color_scale,\n",
    "    title=\"Method\",\n",
    "    legend=alt.Legend(\n",
    "        orient=\"bottom\",\n",
    "        direction=\"horizontal\",\n",
    "        titleAnchor=\"start\",\n",
    "        columns=4,\n",
    "        symbolOpacity=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "ps_inf_df = ps_test_profile_df[\n",
    "    ps_test_profile_df[\"Action\"] == \"[Learner]ProtosegUnet.prediction\"\n",
    "].drop(columns=[\"index\", \"Action\", \"Percentage (%)\"])\n",
    "ps_inf_df[\"Mean per Image (s)\"] = ps_inf_df[\"Mean (s)\"] / ps_inf_df[\"batch_size\"]\n",
    "ps_inf_df[\"Std per Image (s)\"] = ps_inf_df[\"Std (s)\"] / ps_inf_df[\"batch_size\"]\n",
    "ps_inf_df = combine_mean_variance(\n",
    "    ps_inf_df,\n",
    "    groupby_cols=[\"method\", \"batch_size\"],\n",
    "    mean_col=\"Mean per Image (s)\",\n",
    "    std_col=\"Std per Image (s)\",\n",
    "    num_items_col=\"Num Calls\",\n",
    ")\n",
    "ps_inf_df = calc_confidence_limits(ps_inf_df, \"Mean per Image (s)\", \"Std per Image (s)\")\n",
    "\n",
    "ps_sl_inf_df = pd.concat(\n",
    "    [\n",
    "        sl_inf_df[sl_inf_df[\"batch_size\"] <= 16].drop(\n",
    "            columns=[\"shot\", \"Mean (s)\", \"Std (s)\", \"Sum (s)\"]\n",
    "        ),\n",
    "        ps_inf_df,\n",
    "    ]\n",
    ")\n",
    "ps_sl_inf_df[\"method_parent\"] = \"ProtoSeg\"\n",
    "\n",
    "ws_inf_df = ws_test_profile_df[\n",
    "    ws_test_profile_df[\"Action\"] == \"[Learner]WeaselUnet.prediction\"\n",
    "].drop(columns=[\"index\", \"Action\", \"Percentage (%)\"])\n",
    "ws_inf_df[\"Mean per Image (s)\"] = ws_inf_df[\"Mean (s)\"] / ws_inf_df[\"batch_size\"]\n",
    "ws_inf_df[\"Std per Image (s)\"] = ws_inf_df[\"Std (s)\"] / ws_inf_df[\"batch_size\"]\n",
    "ws_inf_df = combine_mean_variance(\n",
    "    ws_inf_df,\n",
    "    groupby_cols=[\"method\", \"batch_size\"],\n",
    "    mean_col=\"Mean per Image (s)\",\n",
    "    std_col=\"Std per Image (s)\",\n",
    "    num_items_col=\"Num Calls\",\n",
    ")\n",
    "ws_inf_df = calc_confidence_limits(ws_inf_df, \"Mean per Image (s)\", \"Std per Image (s)\")\n",
    "ws_inf_df[\"CL 95 L\"] = ws_inf_df[\"CL 95 L\"].clip(lower=0)\n",
    "\n",
    "ws_sl_inf_df = pd.concat(\n",
    "    [\n",
    "        sl_inf_df[sl_inf_df[\"batch_size\"] <= 16].drop(\n",
    "            columns=[\"shot\", \"Mean (s)\", \"Std (s)\", \"Sum (s)\"]\n",
    "        ),\n",
    "        ws_inf_df,\n",
    "    ]\n",
    ")\n",
    "ws_sl_inf_df[\"method_parent\"] = \"WeaSeL\"\n",
    "\n",
    "all_inf_df = pd.concat([ps_sl_inf_df, ws_sl_inf_df])\n",
    "all_inf_df = all_inf_df[all_inf_df[\"batch_size\"] >= 4]\n",
    "\n",
    "line_chart = (\n",
    "    alt.Chart(all_inf_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=\"Mean per Image (s)\",\n",
    "        color=color,\n",
    "    )\n",
    ")\n",
    "\n",
    "error_chart = (\n",
    "    alt.Chart(all_inf_df)\n",
    "    .mark_errorband(opacity=0.2)\n",
    "    .encode(\n",
    "        x=alt.X(\"batch_size\").title(\"Batch Size\"),\n",
    "        y=alt.Y(\"CL 95 U\").title(\"Time per Image (s)\"),\n",
    "        y2=\"CL 95 L\",\n",
    "        color=color,\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    (error_chart + line_chart)\n",
    "    .properties(width=335, height=200)\n",
    "    .facet(row=alt.Row(\"method_parent\", header=alt.Header(title=None)), spacing=10)\n",
    "    .resolve_scale(y=\"independent\")\n",
    "    .configure_axis(labelFontSize=14, titleFontSize=14)\n",
    "    .configure_header(labelFontSize=14, titleFontSize=14)\n",
    "    .configure_legend(labelFontSize=14, titleFontSize=16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = alt.Color(\n",
    "    \"method:N\",\n",
    "    scale=color_scale_ps,\n",
    "    title=\"Method\",\n",
    "    legend=alt.Legend(\n",
    "        orient=\"bottom\",\n",
    "        direction=\"horizontal\",\n",
    "        titleAnchor=\"start\",\n",
    "        columns=4,\n",
    "        symbolOpacity=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "ps_inf_df = ps_test_profile_df[\n",
    "    ps_test_profile_df[\"Action\"] == \"[Learner]ProtosegUnet.post_process\"\n",
    "].drop(columns=[\"index\", \"Action\", \"Percentage (%)\"])\n",
    "ps_inf_df[\"Mean per Image (s)\"] = ps_inf_df[\"Mean (s)\"] / ps_inf_df[\"batch_size\"]\n",
    "ps_inf_df[\"Std per Image (s)\"] = ps_inf_df[\"Std (s)\"] / ps_inf_df[\"batch_size\"]\n",
    "ps_inf_df = combine_mean_variance(\n",
    "    ps_inf_df,\n",
    "    groupby_cols=[\"method\", \"batch_size\"],\n",
    "    mean_col=\"Mean per Image (s)\",\n",
    "    std_col=\"Std per Image (s)\",\n",
    "    num_items_col=\"Num Calls\",\n",
    ")\n",
    "ps_inf_df = calc_confidence_limits(ps_inf_df, \"Mean per Image (s)\", \"Std per Image (s)\")\n",
    "\n",
    "ps_inf_df = ps_inf_df[ps_inf_df[\"batch_size\"] >= 4]\n",
    "\n",
    "line_chart = (\n",
    "    alt.Chart(ps_inf_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=\"batch_size\",\n",
    "        y=\"Mean per Image (s)\",\n",
    "        color=color,\n",
    "    )\n",
    ")\n",
    "\n",
    "error_chart = (\n",
    "    alt.Chart(ps_inf_df)\n",
    "    .mark_errorband(opacity=0.2)\n",
    "    .encode(\n",
    "        x=alt.X(\"batch_size\").title(\"Batch Size\"),\n",
    "        y=alt.Y(\"CL 95 U\").title(\"Time per Image (s)\"),\n",
    "        y2=\"CL 95 L\",\n",
    "        color=color,\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    (line_chart + error_chart)\n",
    "    .properties(width=335, height=200)\n",
    "    .configure_axis(labelFontSize=14, titleFontSize=14)\n",
    "    .configure_legend(labelFontSize=14, titleFontSize=16)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_two_methods_time_paired(\n",
    "#     faster_method: str, slower_method: str, action_name: str\n",
    "# ) -> tuple[float, float]:\n",
    "#     df = test_profile_df[test_profile_df[\"Action\"] == action_name]\n",
    "#     df = df.drop(\n",
    "#         columns=[\"index\", \"Action\", \"Std (s)\", \"Num Calls\", \"Sum (s)\", \"Percentage (%)\"]\n",
    "#     )\n",
    "#     slower_df = df[df[\"method\"] == slower_method]\n",
    "#     faster_df = df[df[\"method\"] == faster_method]\n",
    "\n",
    "#     merged_df = pd.merge(\n",
    "#         slower_df, faster_df, on=[\"batch_size\", \"shot\"], suffixes=(\" S\", \" F\")\n",
    "#     )\n",
    "#     n = len(merged_df)\n",
    "\n",
    "#     merged_df[\"diff\"] = merged_df[\"Mean (s) F\"] - merged_df[\"Mean (s) S\"]\n",
    "\n",
    "#     t_value = merged_df[\"diff\"].mean() / (merged_df[\"diff\"].std() / np.sqrt(n))\n",
    "#     p_value = stats.t.cdf(t_value, n - 1)\n",
    "#     assert isinstance(p_value, float)\n",
    "\n",
    "#     return t_value, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_two_methods_time_paired(\n",
    "#     \"EO-ProtoSeg\", \"ProtoSeg\", \"[Learner]ProtosegUnet.evaluation_process\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_two_methods_time_paired(\n",
    "#     \"EO-WeaSeL\", \"WeaSeL\", \"[Learner]WeaselUnet.evaluation_process\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_two_methods_time(\n",
    "    faster_method: str, slower_method: str, action_name: str\n",
    ") -> tuple[float, float]:\n",
    "    def combine_variance(df, mean_value):\n",
    "        within = ((df[\"Num Calls\"] - 1) * df[\"Std (s)\"] ** 2).sum()\n",
    "        between = (df[\"Num Calls\"] * (df[\"Mean (s)\"] - mean_value) ** 2).sum()\n",
    "        return (within + between) / df[\"Num Calls\"].sum()\n",
    "\n",
    "    df = test_profile_df[test_profile_df[\"Action\"] == action_name]\n",
    "    df = df.drop(columns=[\"index\", \"Action\", \"Percentage (%)\"])\n",
    "    slower_df = df[df[\"method\"] == slower_method]\n",
    "    faster_df = df[df[\"method\"] == faster_method]\n",
    "\n",
    "    slower_n = slower_df[\"Num Calls\"].sum()\n",
    "    faster_n = faster_df[\"Num Calls\"].sum()\n",
    "\n",
    "    slower_mean = slower_df[\"Sum (s)\"].sum() / slower_n\n",
    "    faster_mean = faster_df[\"Sum (s)\"].sum() / faster_n\n",
    "\n",
    "    slower_var = combine_variance(slower_df, slower_mean) / slower_n\n",
    "    faster_var = combine_variance(faster_df, faster_mean) / faster_n\n",
    "\n",
    "    t_value = (faster_mean - slower_mean) / ((faster_var + slower_var) ** 0.5)\n",
    "    dof = ((faster_var + slower_var) ** 2) / (\n",
    "        (faster_var**2) / (faster_n - 1) + (slower_var**2) / (slower_n - 1)\n",
    "    )\n",
    "    p_value = stats.t.cdf(t_value, dof)\n",
    "    assert isinstance(p_value, float)\n",
    "\n",
    "    return t_value, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_two_methods_time(\"EO-ProtoSeg\", \"ProtoSeg\", \"[Learner]ProtosegUnet.get_prototypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_two_methods_time(\"EO-ProtoSeg\", \"ProtoSeg\", \"[Learner]ProtosegUnet.prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_two_methods_time(\"EO-ProtoSeg\", \"ProtoSeg\", \"[Learner]ProtosegUnet.post_process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_two_methods_time(\"EO-WeaSeL\", \"WeaSeL\", \"[Learner]WeaselUnet.tune_process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_two_methods_time(\"EO-WeaSeL\", \"WeaSeL\", \"[Learner]WeaselUnet.prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Other Studies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [\"CFEA [86]\", \"UDA\", None, 79.78, 70.52, 88.96, 75.86, 60.08, 46.53],\n",
    "    [\"pOSAL [25]\", \"UDA\", \"5.8M\", 91.42, 72.30, 90.83, 78.31, 76.75, 62.59],\n",
    "    [\"SIFA [182]\", \"UDA\", \"43.3M\", 83.04, 57.29, 85.69, 69.57, 74.67, 52.84],\n",
    "    [\"WGAN [77]\", \"UDA\", None, 91.20, 72.40, None, None, None, None],\n",
    "    [\"IOSUDA [88]\", \"UDA\", \"42.8M\", 89.53, 65.56, 91.04, 71.03, 83.26, 60.07],\n",
    "    [\"CADA [87]\", \"UDA\", \"9.7M\", 80.18, 72.41, 90.44, 77.21, 62.13, 47.1],\n",
    "    [\"SCUDA [90]\", \"UDA\", None, 90.34, 66.61, None, None, 84.89, 61.65],\n",
    "    [\"GrabCut+UNet [65]\", \"WSS\", None, 86.37, None, None, None, None, None],\n",
    "    [\"MERU [176]\", \"FSS\", None, None, None, 83.92, 61.47, None, None],\n",
    "    [\"RDMT [178]\", \"SSS\", None, None, None, None, 70.93, None, None],\n",
    "    [\"EO-ProtoSeg 1s\", \"FWS\", \"1.9M\", 84.96, 63.69, 88.15, 71.17, 79.92, 44.01],\n",
    "    [\"EO-ProtoSeg 5s\", \"FWS\", \"1.9M\", 85.30, 68.61, 88.18, 73.11, 80.46, 50.27],\n",
    "    [\"EO-ProtoSeg 10s\", \"FWS\", \"1.9M\", 85.02, 68.93, 88.18, 73.52, 80.57, 52.42],\n",
    "    [\"EO-ProtoSeg best\", \"FWS\", \"1.9M\", 86.80, 71.78, 88.21, 73.70, 80.39, 52.65],\n",
    "]\n",
    "\n",
    "columns = [\n",
    "    \"method\",\n",
    "    \"method_type\",\n",
    "    \"params\",\n",
    "    \"drishti_od\",\n",
    "    \"drishti_oc\",\n",
    "    \"refuge_od\",\n",
    "    \"refuge_oc\",\n",
    "    \"rimone_od\",\n",
    "    \"rimone_oc\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df = df[~df[\"method\"].str.contains(\"EO-ProtoSeg\")]\n",
    "\n",
    "other_df.mean(axis=0, numeric_only=True, skipna=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df_diff = other_df.copy()\n",
    "for col in other_df.columns[3:]:\n",
    "    other_df_diff[col] = other_df_diff[col] - df.iloc[-1][col]\n",
    "\n",
    "other_df_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# from utils.wandb import wandb_path\n",
    "\n",
    "# runs = wandb.Api().runs(\n",
    "#     wandb_path(False),\n",
    "#     filters={\"tags\": \"var_region\"},\n",
    "# )\n",
    "\n",
    "# for i, run in enumerate(runs):\n",
    "#     run_id = run.name.split(\" \")[-1]\n",
    "#     group = run.group\n",
    "#     segments = run.config[\"region_segments\"]\n",
    "#     compactness = run.config[\"region_compactness\"]\n",
    "#     compactness = round(np.log10(compactness), 2)\n",
    "#     artifacts = run.logged_artifacts()\n",
    "#     selected_artifact = artifacts[len(artifacts) - 2]\n",
    "#     selected_artifact.download(f\"logs/wandb/1_region_metrics/{group} {segments} {compactness} {run_id}\")\n",
    "#     print(group, segments, compactness, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# df_list = []\n",
    "\n",
    "# wandb_dir = \"logs/wandb/1_region_metrics\"\n",
    "# for dir in os.listdir(wandb_dir):\n",
    "#     if os.path.isfile(f\"{wandb_dir}/{dir}\"):\n",
    "#         continue\n",
    "#     _, segments, compactness, _ = dir.split(\" \")\n",
    "#     df = read_wandb_table(f\"{wandb_dir}/{dir}/metrics.table.json\")\n",
    "#     assert sum(df[\"type\"] == \"TS\") == len(df)\n",
    "#     df.drop(columns=[\"type\", \"epoch\"], inplace=True)\n",
    "#     df.insert(0, \"compactness\", 10**(float(compactness)))\n",
    "#     df.insert(0, \"segments\", int(segments))\n",
    "#     df_list.append(df)\n",
    "\n",
    "# region_metrics_df = pd.concat(df_list)\n",
    "# region_metrics_df.to_csv(\"logs/wandb/1_region_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_coverage_df = pd.read_csv(\"logs/region_coverage.csv\")\n",
    "\n",
    "region_coverage_df[\"ratio_segments\"] = (\n",
    "    region_coverage_df[\"covered_segments\"] / region_coverage_df[\"total_segments\"]\n",
    ")\n",
    "region_coverage_df[\"ratio_class_1\"] = (\n",
    "    region_coverage_df[\"covered_class_1\"] / region_coverage_df[\"total_class_1\"]\n",
    ")\n",
    "region_coverage_df[\"ratio_class_2\"] = (\n",
    "    region_coverage_df[\"covered_class_2\"] / region_coverage_df[\"total_class_2\"]\n",
    ")\n",
    "\n",
    "region_coverage_df = (\n",
    "    region_coverage_df.groupby([\"segments\", \"compactness\"])[\n",
    "        [\"ratio_segments\", \"ratio_class_1\", \"ratio_class_2\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(len(region_coverage_df))\n",
    "region_coverage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_metrics_df = pd.read_csv(\"logs/wandb/1_region_metrics.csv\")\n",
    "\n",
    "region_metrics_df = (\n",
    "    region_metrics_df.groupby([\"segments\", \"compactness\"])[[\"iou_cup\", \"iou_disc\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "region_metrics_df[\"iou\"] = (\n",
    "    region_metrics_df[\"iou_cup\"] + region_metrics_df[\"iou_disc\"]\n",
    ") / 2\n",
    "\n",
    "print(len(region_metrics_df))\n",
    "region_metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df = pd.merge(\n",
    "    region_coverage_df,\n",
    "    region_metrics_df,\n",
    "    on=[\"segments\", \"compactness\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "region_df = region_df.rename(\n",
    "    columns={\n",
    "        \"ratio_class_1\": \"coverage_cup\",\n",
    "        \"ratio_class_2\": \"coverage_disc\",\n",
    "    }\n",
    ")\n",
    "\n",
    "for col in [\n",
    "    \"ratio_segments\",\n",
    "    \"coverage_cup\",\n",
    "    \"coverage_disc\",\n",
    "    \"iou_cup\",\n",
    "    \"iou_disc\",\n",
    "    \"iou\",\n",
    "]:\n",
    "    region_df[col] = region_df[col] * 100\n",
    "\n",
    "print(len(region_df))\n",
    "region_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(region_df).mark_circle(size=60).encode(\n",
    "    x=alt.X(\"segments\"),\n",
    "    y=alt.Y(\"iou_disc\", scale=alt.Scale(domain=[81, 87])),\n",
    "    # y=alt.Y(\"iou_cup\", scale=alt.Scale(domain=[45, 65])),\n",
    "    color=alt.Color(\"coverage_disc\", scale=alt.Scale(scheme=\"viridis\")),\n",
    "    # color=alt.Color(\"coverage_cup\", scale=alt.Scale(scheme=\"viridis\"))\n",
    ").properties(\n",
    "    width=150,\n",
    "    height=200,\n",
    ").facet(column=alt.Column(\"compactness\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = {\"height\": 200, \"width\": 200}\n",
    "\n",
    "color_legend = alt.Legend(orient=\"bottom\", titleOrient=\"left\")\n",
    "\n",
    "line_segments = (\n",
    "    alt.Chart(region_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=alt.X(\"compactness\", scale=alt.Scale(type=\"log\")),\n",
    "        y=alt.Y(\n",
    "            \"ratio_segments\",\n",
    "            scale=alt.Scale(domain=[40, 90]),\n",
    "            title=\"Valid Segments (%)\",\n",
    "        ),\n",
    "        color=alt.Color(\n",
    "            \"segments\", scale=alt.Scale(scheme=\"yelloworangered\"), legend=color_legend\n",
    "        ),\n",
    "        strokeWidth=alt.value(2.5),\n",
    "    )\n",
    "    .properties(**props)\n",
    ")\n",
    "\n",
    "line_disc = (\n",
    "    alt.Chart(region_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=alt.X(\"compactness\", scale=alt.Scale(type=\"log\")),\n",
    "        y=alt.Y(\n",
    "            \"coverage_disc\",\n",
    "            scale=alt.Scale(domain=[0, 70]),\n",
    "            title=\"Coverage of OD Pixels (%)\",\n",
    "        ),\n",
    "        color=alt.Color(\n",
    "            \"segments\", scale=alt.Scale(scheme=\"yelloworangered\"), legend=color_legend\n",
    "        ),\n",
    "        strokeWidth=alt.value(2.5),\n",
    "    )\n",
    "    .properties(**props)\n",
    ")\n",
    "\n",
    "line_cup = (\n",
    "    alt.Chart(region_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=alt.X(\"compactness\", scale=alt.Scale(type=\"log\")),\n",
    "        y=alt.Y(\n",
    "            \"coverage_cup\",\n",
    "            scale=alt.Scale(domain=[0, 70]),\n",
    "            title=\"Coverage of OC Pixels (%)\",\n",
    "        ),\n",
    "        color=alt.Color(\n",
    "            \"segments\", scale=alt.Scale(scheme=\"yelloworangered\"), legend=color_legend\n",
    "        ),\n",
    "        strokeWidth=alt.value(2.5),\n",
    "    )\n",
    "    .properties(**props)\n",
    ")\n",
    "\n",
    "scatter_segments = (\n",
    "    alt.Chart(region_df)\n",
    "    .mark_circle(size=40)\n",
    "    .encode(\n",
    "        x=alt.X(\n",
    "            \"ratio_segments\",\n",
    "            scale=alt.Scale(domain=[40, 90]),\n",
    "            title=\"Valid Segments (%)\",\n",
    "        ),\n",
    "        y=alt.Y(\"mean(iou)\", scale=alt.Scale(domain=[50, 80]), title=\"Mean IoU (%)\"),\n",
    "        color=alt.value(\"gray\"),\n",
    "    )\n",
    "    .properties(**props)\n",
    ")\n",
    "\n",
    "scatter_disc = (\n",
    "    alt.Chart(region_df)\n",
    "    .mark_circle(size=40)\n",
    "    .encode(\n",
    "        x=alt.X(\n",
    "            \"coverage_disc\",\n",
    "            scale=alt.Scale(domain=[0, 70]),\n",
    "            title=\"Coverage of OD Pixels (%)\",\n",
    "        ),\n",
    "        y=alt.Y(\"mean(iou_disc)\", scale=alt.Scale(domain=[60, 90]), title=\"OD IoU (%)\"),\n",
    "        color=alt.value(\"gray\"),\n",
    "    )\n",
    "    .properties(**props)\n",
    ")\n",
    "\n",
    "scatter_cup = (\n",
    "    alt.Chart(region_df)\n",
    "    .mark_circle(size=40)\n",
    "    .encode(\n",
    "        x=alt.X(\n",
    "            \"coverage_cup\",\n",
    "            scale=alt.Scale(domain=[0, 70]),\n",
    "            title=\"Coverage of OC Pixels (%)\",\n",
    "        ),\n",
    "        y=alt.Y(\"mean(iou_cup)\", scale=alt.Scale(domain=[40, 70]), title=\"OC IoU (%)\"),\n",
    "        color=alt.value(\"gray\"),\n",
    "    )\n",
    "    .properties(**props)\n",
    ")\n",
    "\n",
    "(\n",
    "    (\n",
    "        (line_segments & line_disc & line_cup)\n",
    "        | (scatter_segments & scatter_disc & scatter_cup)\n",
    "    )\n",
    "    .configure_axis(labelFontSize=12, titleFontSize=14, titleFontWeight=500)\n",
    "    .configure_legend(labelFontSize=14, titleFontSize=14, titleFontWeight=500)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
