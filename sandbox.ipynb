{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from data.dummy_dataset import DummyDataset, DummySimpleDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_simple_dataset = DummySimpleDataset(\n",
    "    \"val\",\n",
    "    3,\n",
    "    (256, 256),\n",
    "    max_items=100,\n",
    "    seed=0,\n",
    "    split_val_size=0.2,\n",
    "    split_test_size=0.2,\n",
    "    dataset_name=\"Dummy\",\n",
    ")\n",
    "\n",
    "print(len(dummy_simple_dataset))\n",
    "img, msk, name, _ = dummy_simple_dataset[79]\n",
    "\n",
    "print(\n",
    "    name,\n",
    "    img.shape,\n",
    "    img.dtype,\n",
    "    img.min(),\n",
    "    img.max(),\n",
    "    msk.shape,\n",
    "    msk.dtype,\n",
    "    torch.unique(msk),\n",
    ")\n",
    "\n",
    "plt.imshow(np.moveaxis(img.numpy(), 0, -1))\n",
    "# plt.imshow(msk.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dataset = DummyDataset(\n",
    "    \"train\",\n",
    "    3,\n",
    "    (256, 256),\n",
    "    max_items=100,\n",
    "    seed=0,\n",
    "    split_val_size=0.2,\n",
    "    split_test_size=0.2,\n",
    "    dataset_name=\"Dummy\",\n",
    "    shot_options=[1, 5, 10, 20],\n",
    "    # shot_options=\"all\",\n",
    "    sparsity_options=[\n",
    "        (\"point\", [1, 5, 10, 20]),\n",
    "        # (\"grid\", (10, 20)),\n",
    "        # (\"contour\", \"random\"),\n",
    "        # (\"skeleton\", (0.1, 0.5)),\n",
    "        # (\"region\", 0.5),\n",
    "    ],\n",
    "    shot_sparsity_permutation=True,\n",
    "    num_iterations=1.0,\n",
    "    query_batch_size=5,\n",
    "    split_query_size=0.9,\n",
    ")\n",
    "\n",
    "print(len(dummy_dataset))\n",
    "support, query, _ = dummy_dataset[0]\n",
    "\n",
    "supp_img, supp_msk, supp_name, supp_sparsity_mode, supp_sparsity_value = support\n",
    "qry_img, qry_msk, qry_name = query\n",
    "\n",
    "print(\n",
    "    supp_img.shape,\n",
    "    supp_img.dtype,\n",
    "    supp_img.min(),\n",
    "    supp_img.max(),\n",
    "    supp_msk.shape,\n",
    "    supp_msk.dtype,\n",
    "    torch.unique(supp_msk),\n",
    ")\n",
    "print(supp_name, supp_sparsity_mode, supp_sparsity_value)\n",
    "print(\n",
    "    qry_img.shape,\n",
    "    qry_img.dtype,\n",
    "    qry_img.min(),\n",
    "    qry_img.max(),\n",
    "    qry_msk.shape,\n",
    "    qry_msk.dtype,\n",
    "    torch.unique(qry_msk),\n",
    ")\n",
    "print(qry_name)\n",
    "print()\n",
    "\n",
    "# plt.imshow(supp_msk[0].numpy())\n",
    "# plt.imshow(qry_msk[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "dummy_loader = DataLoader(\n",
    "    ConcatDataset([dummy_dataset]),\n",
    "    batch_size=None,\n",
    "    shuffle=dummy_dataset.mode == \"train\",\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "for batch in dummy_loader:\n",
    "    support, query, dataset_name = batch\n",
    "\n",
    "    print(type(batch.support))\n",
    "    print(support[0].shape, support[1].shape, support[2][:4], support[3])\n",
    "    print(query[0].shape, query[1].shape, query[2])\n",
    "    print(dataset_name)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dummy_dataset.support_batches)\n",
    "for i in range(len(dummy_dataset)):\n",
    "    support, query, _ = dummy_dataset[i]\n",
    "    supp_img, supp_msk, supp_name, supp_sparsity_mode, supp_sparsity_value = support\n",
    "    qry_img, qry_msk, qry_name = query\n",
    "    print(\n",
    "        supp_img.shape[0],\n",
    "        supp_msk.shape[0],\n",
    "        len(supp_name),\n",
    "        supp_sparsity_mode,\n",
    "        supp_sparsity_value,\n",
    "        qry_img.shape[0],\n",
    "        qry_msk.shape[0],\n",
    "        len(qry_name),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.optic_disc_cup.datasets import RimOneDataset\n",
    "\n",
    "\n",
    "rim_one = RimOneDataset(\n",
    "    \"train\",\n",
    "    3,\n",
    "    (256, 256),\n",
    "    max_items=100,\n",
    "    seed=0,\n",
    "    split_val_size=0.2,\n",
    "    split_test_size=0.2,\n",
    "    dataset_name=\"RIM-ONE DL\",\n",
    "    shot_options=[1, 5, 10, 20],\n",
    "    sparsity_options=[\n",
    "        (\"point\", [1, 5, 10, 20]),\n",
    "        (\"grid\", [20, 10]),\n",
    "        (\"contour\", [0.1, 0.5, 1]),\n",
    "    ],\n",
    "    shot_sparsity_permutation=True,\n",
    "    query_batch_size=2,\n",
    "    split_query_size=0.5,\n",
    ")\n",
    "\n",
    "# for i in range(rim_one.num_iterations):\n",
    "#     print(rim_one.support_batches[i], rim_one.support_sparsities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from learners.metrics import MultiIoUMetric\n",
    "\n",
    "\n",
    "metric = MultiIoUMetric()\n",
    "\n",
    "metric(torch.tensor([1, 0, 1]), torch.tensor([1, 0, 1]))\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.diff_dict import diff_dict\n",
    "\n",
    "\n",
    "diff_dict(\n",
    "    {\n",
    "        \"config\": {},\n",
    "        \"a\": {\"a\": {\"p\": 1}},\n",
    "        \"b\": 2,\n",
    "        \"p\": (12, 34),\n",
    "        \"d\": {\"e\": 4, \"f\": 5, \"g\": [6, 7, 8]},\n",
    "        \"g\": [6, 7, 8],\n",
    "    },\n",
    "    {\n",
    "        \"config\": {},\n",
    "        \"a\": {\"a\": {\"p\": 1, \"q\": 0}},\n",
    "        \"c\": 3,\n",
    "        \"d\": {\"e\": 4, \"f\": 5, \"g\": [6, 9, 8, {\"a\": 1}]},\n",
    "        \"g\": [6, 9],\n",
    "        \"p\": (12, 33, 34),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "for item in os.listdir(\"logs/SL\"):\n",
    "    mtime = os.path.getmtime(os.path.join(\"logs/SL\", item))\n",
    "    print(datetime.datetime.fromtimestamp(mtime).isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_sample_indices(\n",
    "    population_size: int, sample_size: int, batch_size: int\n",
    ") -> list[list[int]]:\n",
    "    import random\n",
    "\n",
    "    samples = sorted(random.sample(range(population_size), sample_size))\n",
    "    population_batch_size = population_size // batch_size + 1\n",
    "    batch_samples = [[] for _ in range(population_batch_size)]\n",
    "    for s in samples:\n",
    "        batch_samples[s // batch_size].append(s - (s // batch_size) * batch_size)\n",
    "    return batch_samples\n",
    "\n",
    "\n",
    "make_batch_sample_indices(100, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_sample_indices_multi(\n",
    "    iterations_batches: list[tuple[int, int]], total_samples: int\n",
    ") -> list[list[int]]:\n",
    "    import random\n",
    "\n",
    "    populations = [iter * batch for iter, batch in iterations_batches]\n",
    "\n",
    "    sum_populations = sum(populations)\n",
    "    samples = [round(p * total_samples / sum_populations) for p in populations]\n",
    "    while True:\n",
    "        sum_samples = sum(samples)\n",
    "        if sum_samples == total_samples:\n",
    "            break\n",
    "        index = random.randint(0, len(samples) - 1)\n",
    "        samples[index] += 1 if sum_samples < total_samples else -1\n",
    "\n",
    "    batch_samples = []\n",
    "    zipped = zip(iterations_batches, populations, samples)\n",
    "    for (_, batch), population, sample in zipped:\n",
    "        batch_samples += make_batch_sample_indices(\n",
    "            population,\n",
    "            sample,\n",
    "            batch,\n",
    "        )\n",
    "\n",
    "    return batch_samples\n",
    "\n",
    "\n",
    "make_batch_sample_indices_multi([(5, 3), (4, 2), (10, 1)], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WandB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from config.constants import WANDB_SETTINGS\n",
    "from utils.wandb import wandb_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_log_dataset_ref(dataset_path: str, dataset_name: str, dummy: bool = False):\n",
    "    wandb_login()\n",
    "    wandb.init(\n",
    "        tags=[\"helper\"],\n",
    "        project=WANDB_SETTINGS[\"dummy_project\" if dummy else \"project\"],\n",
    "        name=f\"log dataset {dataset_name}\",\n",
    "    )\n",
    "    dataset_artifact = wandb.Artifact(dataset_name, type=\"dataset\")\n",
    "    dataset_artifact.add_reference(f\"file://{dataset_path}\")\n",
    "    wandb.log_artifact(dataset_artifact)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "# wandb_log_dataset_ref(\"D:/Penelitian/FWS/Data/DRISHTI-GS\", \"DRISHTI\", True)\n",
    "# wandb_log_dataset_ref(\"D:/Penelitian/FWS/Data/RIM-ONE\", \"RIM-ONE\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ac = wandb.Api().artifact_collection(\n",
    "#     \"run_table\", \"pandegaaz/few-shot-weakly-seg-old/run-svgff5kf-metrics\"\n",
    "# )\n",
    "\n",
    "# ac.delete()\n",
    "\n",
    "# for art in ac.artifacts():\n",
    "#     print(art.name, art.id)\n",
    "\n",
    "# art: wandb.Artifact = ac.artifacts()[0]  # type: ignore\n",
    "\n",
    "# print(art.name, art.aliases)\n",
    "\n",
    "# art.download(\"ppp/qqq\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
