{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from data.dummy_dataset import DummyDataset, DummySimpleDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_simple_dataset = DummySimpleDataset(\n",
    "    \"val\",\n",
    "    3,\n",
    "    (256, 256),\n",
    "    max_items=100,\n",
    "    seed=0,\n",
    "    split_val_size=0.2,\n",
    "    split_test_size=0.2,\n",
    "    dataset_name=\"Dummy\",\n",
    ")\n",
    "\n",
    "print(len(dummy_simple_dataset))\n",
    "img, msk, name, _ = dummy_simple_dataset[79]\n",
    "\n",
    "print(\n",
    "    name,\n",
    "    img.shape,\n",
    "    img.dtype,\n",
    "    img.min(),\n",
    "    img.max(),\n",
    "    msk.shape,\n",
    "    msk.dtype,\n",
    "    torch.unique(msk),\n",
    ")\n",
    "\n",
    "plt.imshow(np.moveaxis(img.numpy(), 0, -1))\n",
    "# plt.imshow(msk.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dataset = DummyDataset(\n",
    "    \"train\",\n",
    "    3,\n",
    "    (256, 256),\n",
    "    max_items=100,\n",
    "    seed=0,\n",
    "    split_val_size=0.2,\n",
    "    split_test_size=0.2,\n",
    "    dataset_name=\"Dummy\",\n",
    "    # shot_options=[1, 5, 10, 20],\n",
    "    shot_options=\"all\",\n",
    "    sparsity_options=[\n",
    "        (\"point\", [1, 5, 10, 20]),\n",
    "        (\"grid\", (10, 20)),\n",
    "        (\"contour\", \"random\"),\n",
    "        (\"skeleton\", (0.1, 0.5)),\n",
    "        # (\"region\", 0.5),\n",
    "    ],\n",
    "    # shot_sparsity_permutation=True,\n",
    "    num_iterations=1.0,\n",
    "    query_batch_size=5,\n",
    "    split_query_size=0.9,\n",
    ")\n",
    "\n",
    "print(len(dummy_dataset))\n",
    "support, query, _ = dummy_dataset[0]\n",
    "\n",
    "supp_img, supp_msk, supp_name, supp_sparsity = support\n",
    "qry_img, qry_msk, qry_name = query\n",
    "\n",
    "print(\n",
    "    supp_img.shape,\n",
    "    supp_img.dtype,\n",
    "    supp_img.min(),\n",
    "    supp_img.max(),\n",
    "    supp_msk.shape,\n",
    "    supp_msk.dtype,\n",
    "    torch.unique(supp_msk),\n",
    ")\n",
    "print(supp_name, supp_sparsity)\n",
    "print(\n",
    "    qry_img.shape,\n",
    "    qry_img.dtype,\n",
    "    qry_img.min(),\n",
    "    qry_img.max(),\n",
    "    qry_msk.shape,\n",
    "    qry_msk.dtype,\n",
    "    torch.unique(qry_msk),\n",
    ")\n",
    "print(qry_name)\n",
    "print()\n",
    "\n",
    "# plt.imshow(supp_msk[0].numpy())\n",
    "# plt.imshow(qry_msk[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dummy_dataset.support_batches)\n",
    "for i in range(len(dummy_dataset)):\n",
    "    support, query, _ = dummy_dataset[i]\n",
    "    supp_img, supp_msk, supp_name, supp_sparsity = support\n",
    "    qry_img, qry_msk, qry_name = query\n",
    "    print(\n",
    "        supp_img.shape[0],\n",
    "        supp_msk.shape[0],\n",
    "        len(supp_name),\n",
    "        supp_sparsity,\n",
    "        qry_img.shape[0],\n",
    "        qry_msk.shape[0],\n",
    "        len(qry_name),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.optic_disc_cup.datasets import RimOneDataset\n",
    "\n",
    "\n",
    "rim_one = RimOneDataset(\n",
    "    \"train\",\n",
    "    3,\n",
    "    (256, 256),\n",
    "    max_items=100,\n",
    "    seed=0,\n",
    "    split_val_size=0.2,\n",
    "    split_test_size=0.2,\n",
    "    dataset_name=\"RIM-ONE DL\",\n",
    "    shot_options=[1, 5, 10, 20],\n",
    "    sparsity_options=[\n",
    "        (\"point\", [1, 5, 10, 20]),\n",
    "        (\"grid\", [20, 10]),\n",
    "        (\"contour\", [0.1, 0.5, 1]),\n",
    "    ],\n",
    "    shot_sparsity_permutation=True,\n",
    "    query_batch_size=2,\n",
    "    split_query_size=0.5,\n",
    ")\n",
    "\n",
    "# for i in range(rim_one.num_iterations):\n",
    "#     print(rim_one.support_batches[i], rim_one.support_sparsities[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import notebook\n",
    "\n",
    "for i in notebook.trange(100):\n",
    "    time.sleep(0.05)\n",
    "\n",
    "for i in notebook.trange(100):\n",
    "    time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from learners.metrics import CustomMetric\n",
    "\n",
    "\n",
    "metric = CustomMetric()\n",
    "\n",
    "metric(torch.tensor([1, 0, 1]), torch.tensor([1, 0, 1]))\n",
    "\n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import diff_dict\n",
    "\n",
    "\n",
    "diff_dict(\n",
    "    {\n",
    "        \"a\": {\"a\": {\"p\": 1}},\n",
    "        \"b\": 2,\n",
    "        \"p\": (12, 34),\n",
    "        \"d\": {\"e\": 4, \"f\": 5, \"g\": [6, 7, 8]},\n",
    "        \"g\": [6, 7, 8],\n",
    "    },\n",
    "    {\n",
    "        \"a\": {\"a\": {\"p\": 1, \"q\": 0}},\n",
    "        \"c\": 3,\n",
    "        \"d\": {\"e\": 4, \"f\": 5, \"g\": [6, 9, 8, {\"a\": 1}]},\n",
    "        \"g\": [6, 9],\n",
    "        \"p\": (12, 33, 34),\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "for item in os.listdir(\"logs/SL\"):\n",
    "    mtime = os.path.getmtime(os.path.join(\"logs/SL\", item))\n",
    "    print(datetime.datetime.fromtimestamp(mtime).isoformat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WandB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from config.constants import WANDB_SETTINGS\n",
    "\n",
    "prefix = WANDB_SETTINGS[\"entity\"] + \"/\" + WANDB_SETTINGS[\"project\"] + \"/\"\n",
    "wandb.Api().artifact(prefix + \"wandb-history:v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.logging import prepare_ckpt_path_for_artifact\n",
    "\n",
    "\n",
    "ckpt_path = \"SL/2024-02-12 11-24/epoch=0 val_score=0.33.ckpt\"\n",
    "\n",
    "prepare_ckpt_path_for_artifact(ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from config.constants import WANDB_SETTINGS\n",
    "\n",
    "from utils.wandb import wandb_login\n",
    "\n",
    "\n",
    "def wandb_log_dataset_ref(\n",
    "    dataset_path: str,\n",
    "    dataset_name: str,\n",
    "):\n",
    "    wandb_login()\n",
    "    wandb.init(\n",
    "        tags=[\"helper\"],\n",
    "        project=WANDB_SETTINGS[\"project\"],\n",
    "        name=f\"log dataset {dataset_name}\",\n",
    "    )\n",
    "    dataset_artifact = wandb.Artifact(dataset_name, type=\"dataset\")\n",
    "    dataset_artifact.add_reference(f\"file://{dataset_path}\")\n",
    "    wandb.log_artifact(dataset_artifact)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "wandb_log_dataset_ref(\"D:/Penelitian/FWS/Data/DRISHTI-GS\", \"DRISHTI\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
